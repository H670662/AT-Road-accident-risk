{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# LGBM regressor\n",
    "LGM regressor scorer ofte bra, jeg har valgt Ã¥ bruke denne for Ã¥ se hva vi kan fÃ¥ ut av datasettet uten stÃ¸rre mengde feature engineering.\n",
    "\n",
    "## importere bibloteker"
   ],
   "id": "b4ff438af74554d3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "dca1603cf68a68fe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T01:17:51.916716Z",
     "start_time": "2025-10-29T01:17:51.913095Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from lightgbm import LGBMRegressor, early_stopping, log_evaluation\n",
    "from sklearn.impute import SimpleImputer\n"
   ],
   "id": "f0ca7c9fcd13ccc7",
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T01:17:52.096579Z",
     "start_time": "2025-10-29T01:17:52.078952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a helper module that runs a grid search over FEATURE GROUPS by selecting columns via name patterns.\n",
    "# This avoids editing your existing feature-engineering functions, and lets you toggle whole groups of features.\n",
    "# You can import and run this inside your notebook.\n",
    "#\n",
    "# It will:\n",
    "# - Build engineered features with your existing function (create_engineered_features)\n",
    "# - Define feature groups via column-name regex patterns\n",
    "# - For each combination of groups, select matching columns, build a fresh preprocessing+LGBM pipeline, and KFold-CV evaluate (RMSE)\n",
    "# - Save a CSV of results sorted by best score\n",
    "#\n",
    "# Usage inside your notebook (after defining/using load_data, create_engineered_features):\n",
    "# from feature_set_gridsearch import run_feature_set_grid_search, default_group_patterns\n",
    "# train, test, _ = load_data()\n",
    "# results = run_feature_set_grid_search(train, target_col=\"accident_risk\",\n",
    "#                                       group_patterns=default_group_patterns,\n",
    "#                                       n_splits=5, random_state=42)\n",
    "# results.head()\n",
    "#\n",
    "# To customize, edit default_group_patterns below to match your column prefixes/patterns.\n",
    "# You can also pass your own dict of patterns to the function.\n",
    "#\n",
    "# The results CSV will be saved to /mnt/data/feature_grid_results.csv in this environment.\n",
    "# In your environment it will save to \"feature_grid_results.csv\" in the working directory by default.\n",
    "\n",
    "import os, re, itertools, json, math\n",
    "from typing import Dict, List, Iterable, Tuple, Optional\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector as selector\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor\n",
    "except Exception as e:\n",
    "    # LightGBM may not be installed in this execution env, but will be in your notebook.\n",
    "    LGBMRegressor = None\n",
    "\n",
    "\n",
    "def _column_matches_any(col: str, patterns: Iterable[str]) -> bool:\n",
    "    for pat in patterns:\n",
    "        if re.search(pat, col):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def build_dynamic_preprocessor(X: pd.DataFrame) -> ColumnTransformer:\n",
    "    \"\"\"Infer a simple preprocessor: scale numeric, one-hot categorical.\"\"\"\n",
    "    numeric_selector = selector(dtype_include=np.number)\n",
    "    categorical_selector = selector(dtype_exclude=np.number)\n",
    "\n",
    "    numeric_transformer = Pipeline(steps=[(\"scale\", StandardScaler(with_mean=False))])\n",
    "    categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, numeric_selector),\n",
    "            (\"cat\", categorical_transformer, categorical_selector),\n",
    "        ]\n",
    "    )\n",
    "    return preprocessor\n",
    "\n",
    "\n",
    "def build_lgbm_pipeline(X: pd.DataFrame, random_state: int = 42) -> Pipeline:\n",
    "    \"\"\"Construct a fresh pipeline for each feature subset.\"\"\"\n",
    "    pre = build_dynamic_preprocessor(X)\n",
    "    if LGBMRegressor is None:\n",
    "        raise RuntimeError(\"LightGBM is not available in this environment. Run this inside your notebook.\")\n",
    "    model = LGBMRegressor(\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=-1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    pipe = Pipeline(steps=[(\"preprocess\", pre), (\"regressor\", model)])\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def select_columns_by_groups(\n",
    "    X: pd.DataFrame, include_groups: List[str], group_patterns: Dict[str, List[str]]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Return X with only columns matching the included groups (union). If a group has [], it is ignored.\"\"\"\n",
    "    if not include_groups:\n",
    "        # No groups selected => return empty feature set (caller may handle or skip)\n",
    "        return X.iloc[:, 0:0].copy()\n",
    "\n",
    "    include_cols = set()\n",
    "    for g in include_groups:\n",
    "        pats = group_patterns.get(g, [])\n",
    "        if not pats:\n",
    "            continue\n",
    "        for c in X.columns:\n",
    "            if _column_matches_any(c, pats):\n",
    "                include_cols.add(c)\n",
    "\n",
    "    # If some columns didn't match any pattern, they are implicitly excluded.\n",
    "    if not include_cols:\n",
    "        # Fall back to nothing\n",
    "        return X.iloc[:, 0:0].copy()\n",
    "\n",
    "    return X.loc[:, sorted(include_cols)].copy()\n",
    "\n",
    "\n",
    "def expand_boolean_grid(options: List[str], max_groups: Optional[int] = None) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Generate all group combinations (like a ParameterGrid over True/False per group).\n",
    "    If max_groups is set, only include combos up to that size (ablation-style).\n",
    "    \"\"\"\n",
    "    groups = options\n",
    "    combos = []\n",
    "    for r in range(1, len(groups) + 1):\n",
    "        if max_groups is not None and r > max_groups:\n",
    "            break\n",
    "        for subset in itertools.combinations(groups, r):\n",
    "            combos.append(list(subset))\n",
    "    return combos\n",
    "\n",
    "\n",
    "def run_feature_set_grid_search(\n",
    "    train: pd.DataFrame,\n",
    "    target_col: str,\n",
    "    group_patterns: Dict[str, List[str]],\n",
    "    id_cols: Optional[List[str]] = None,\n",
    "    n_splits: int = 5,\n",
    "    random_state: int = 42,\n",
    "    max_groups: Optional[int] = None,\n",
    "    save_path: str = \"feature_grid_results.csv\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform KFold CV over combinations of feature groups chosen by name-patterns.\n",
    "    Returns a DataFrame with mean/std RMSE and columns used.\n",
    "    \"\"\"\n",
    "    id_cols = id_cols or [\"id\"]\n",
    "    if target_col not in train.columns:\n",
    "        raise ValueError(f\"target_col '{target_col}' not found in train columns\")\n",
    "\n",
    "    # Use your existing feature engineering to create columns up front.\n",
    "    # We import from the current kernel: create_engineered_features was defined in your notebook.\n",
    "    if \"create_engineered_features\" not in globals():\n",
    "        raise RuntimeError(\"Expected 'create_engineered_features' to be defined in the notebook environment.\")\n",
    "    full = create_engineered_features(train.copy())\n",
    "\n",
    "    # Separate target\n",
    "    y = full[target_col].copy()\n",
    "    # Drop target + id columns\n",
    "    drop_cols = [c for c in id_cols if c in full.columns] + [target_col]\n",
    "    X_full = full.drop(columns=drop_cols, errors=\"ignore\")\n",
    "\n",
    "    # Build all group combinations\n",
    "    all_groups = list(group_patterns.keys())\n",
    "    combos = expand_boolean_grid(all_groups, max_groups=max_groups)\n",
    "    if not combos:\n",
    "        raise RuntimeError(\"No group combinations produced. Check your group_patterns.\")\n",
    "\n",
    "    records = []\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    for include_groups in combos:\n",
    "        X_sub = select_columns_by_groups(X_full, include_groups, group_patterns)\n",
    "\n",
    "        if X_sub.shape[1] == 0:\n",
    "            # Skip empty feature sets\n",
    "            continue\n",
    "\n",
    "        rmses = []\n",
    "        for fold, (tr_idx, va_idx) in enumerate(kf.split(X_sub), start=1):\n",
    "            X_tr, X_va = X_sub.iloc[tr_idx], X_sub.iloc[va_idx]\n",
    "            y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "\n",
    "            pipe = build_lgbm_pipeline(X_tr, random_state=random_state)\n",
    "            pipe.fit(X_tr, y_tr)\n",
    "\n",
    "            pred = pipe.predict(X_va)\n",
    "            rmse = mean_squared_error(y_va, pred)\n",
    "            rmses.append(rmse)\n",
    "\n",
    "        records.append({\n",
    "            \"groups\": include_groups,\n",
    "            \"n_features\": X_sub.shape[1],\n",
    "            \"cv_rmse_mean\": float(np.mean(rmses)),\n",
    "            \"cv_rmse_std\": float(np.std(rmses)),\n",
    "            \"fold_rmses\": rmses,\n",
    "        })\n",
    "\n",
    "    results = pd.DataFrame.from_records(records).sort_values(\"cv_rmse_mean\").reset_index(drop=True)\n",
    "    results.to_csv(save_path, index=False)\n",
    "    print(f\"Saved results to {save_path} with {len(results)} rows.\")\n",
    "    return results\n",
    "\n",
    "\n",
    "# === EXAMPLE DEFAULT GROUP PATTERNS ===\n",
    "# Customize this mapping to your engineered feature names.\n",
    "# Use regex patterns; each group is a list of patterns. A column matches a group if it matches ANY pattern in that list.\n",
    "# Tailored feature groups matching your exact columns\n",
    "group_patterns = {\n",
    "    # Categorical\n",
    "    \"categorical\": [\n",
    "        r\"^(road_type|lighting|weather|time_of_day)$\",\n",
    "    ],\n",
    "\n",
    "    # Boolean flags (treated as categorical in the pipeline unless you cast to int)\n",
    "    \"booleans\": [\n",
    "        r\"^(holiday|school_season|road_signs_present|public_road)$\",\n",
    "    ],\n",
    "\n",
    "    # Base numeric\n",
    "    \"base_numeric\": [\n",
    "        r\"^(num_lanes|curvature|speed_limit|num_reported_accidents)$\",\n",
    "    ],\n",
    "\n",
    "    # Core engineered single feature\n",
    "    \"speed_o_curve\": [\n",
    "        r\"^speed_o_curve$\",\n",
    "    ],\n",
    "\n",
    "    # Composite / risk components\n",
    "    \"composite_risk\": [\n",
    "        r\"^(visibility_composite|lighting_risk|weather_risk)$\",\n",
    "    ],\n",
    "\n",
    "    # Time-related\n",
    "    \"time\": [\n",
    "        r\"^time_as_int$\",\n",
    "    ],\n",
    "\n",
    "    # Log transforms\n",
    "    \"logs\": [\n",
    "        r\"^log_speed_o_curve$\",\n",
    "    ],\n",
    "\n",
    "    # Interactions\n",
    "    \"interactions\": [\n",
    "        r\"^(curvature_x_speed|accidents_o_lanes|speed_time_interaction|curvature_time_interaction)$\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# (Optional) If you later add these, keep a parking group:\n",
    "# \"experimental_disabled\": [r\"^(speed_x_speed|speed_x_accidents)$\"]\n",
    "\n",
    "\n",
    "\n",
    "# If run as a script (optional): do nothing. The main use is to import from your notebook.\n",
    "\n"
   ],
   "id": "d3479f806e0d9dff",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T01:17:52.287557Z",
     "start_time": "2025-10-29T01:17:52.280427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from copy import deepcopy\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import clone\n",
    "\n",
    "def _filter_columns(cols, present_cols):\n",
    "    \"\"\"Return cols âˆ© present_cols if cols is a list/array of names; otherwise return cols unchanged.\"\"\"\n",
    "    if cols is None:\n",
    "        return cols\n",
    "    # Lists/tuples/Index of col names\n",
    "    if isinstance(cols, (list, tuple)):\n",
    "        return [c for c in cols if c in present_cols]\n",
    "    try:\n",
    "        # Pandas Index\n",
    "        import pandas as pd\n",
    "        if isinstance(cols, pd.Index):\n",
    "            return cols.intersection(present_cols)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Callables, selectors, slices, etc. leave as-is.\n",
    "    return cols\n",
    "\n",
    "def trim_preprocessor_columns(preprocessor, X_cols):\n",
    "    \"\"\"\n",
    "    Clone a ColumnTransformer-based preprocessor and trim any explicit column name lists\n",
    "    to only those present in X_cols. This avoids KeyErrors when columns are subset.\n",
    "    \"\"\"\n",
    "    pp = clone(preprocessor)  # shallow clone of estimator config\n",
    "    # We need to adjust the underlying ColumnTransformer config\n",
    "    if isinstance(pp, ColumnTransformer):\n",
    "        # Access the raw 'transformers' param (list of (name, transformer, columns))\n",
    "        new_transformers = []\n",
    "        for name, trans, cols in pp.transformers:\n",
    "            new_cols = _filter_columns(cols, set(X_cols))\n",
    "            new_transformers.append((name, trans, new_cols))\n",
    "        pp.set_params(transformers=new_transformers)\n",
    "    else:\n",
    "        # If your preprocessor is wrapped inside a Pipeline, unwrap/handle as needed\n",
    "        try:\n",
    "            from sklearn.pipeline import Pipeline\n",
    "            if isinstance(pp, Pipeline):\n",
    "                steps = []\n",
    "                for nm, step in pp.steps:\n",
    "                    if isinstance(step, ColumnTransformer):\n",
    "                        # trim this step\n",
    "                        step_trimmed = trim_preprocessor_columns(step, X_cols)\n",
    "                        steps.append((nm, step_trimmed))\n",
    "                    else:\n",
    "                        steps.append((nm, step))\n",
    "                pp.set_params(steps=steps)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return pp\n"
   ],
   "id": "1889dedf5f35a2b7",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T01:17:52.510308Z",
     "start_time": "2025-10-29T01:17:52.497481Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def run_feature_set_grid_search_fold_safe(\n",
    "    train: pd.DataFrame,\n",
    "    target_col: str,\n",
    "    group_patterns: dict,\n",
    "    id_cols=None,\n",
    "    n_splits: int = 5,\n",
    "    random_state: int = 42,\n",
    "    max_groups=None,\n",
    "    save_path: str = \"feature_grid_results_foldsafe.csv\",\n",
    "    verbose: bool = True,\n",
    "    save_every: int = 0,          # e.g., 10 to save partial results every 10 combos; 0 disables\n",
    "):\n",
    "    \"\"\"\n",
    "    KFold CV with per-fold feature engineering to avoid leakage:\n",
    "      - Split RAW train -> (tr, va)\n",
    "      - Run create_engineered_features(tr) and (...va) independently\n",
    "      - Select columns by group patterns *based on the train-fold columns*\n",
    "      - Fit pipeline on train-fold, evaluate on val-fold\n",
    "\n",
    "    Progress logging:\n",
    "      - Prints progress \"[i/total]\" per combination if verbose=True\n",
    "      - Prints per-fold RMSEs and final mean/std per combo\n",
    "      - Optionally saves partial CSV every `save_every` combos\n",
    "    \"\"\"\n",
    "    id_cols = id_cols or [\"id\"]\n",
    "    if target_col not in train.columns:\n",
    "        raise ValueError(f\"target_col '{target_col}' not in raw train\")\n",
    "\n",
    "    if \"create_engineered_features\" not in globals():\n",
    "        raise RuntimeError(\"Expected 'create_engineered_features' to be defined in the notebook.\")\n",
    "\n",
    "    all_groups = list(group_patterns.keys())\n",
    "    combos = expand_boolean_grid(all_groups, max_groups=max_groups)\n",
    "    if not combos:\n",
    "        raise RuntimeError(\"No group combinations produced. Check your group_patterns.\")\n",
    "\n",
    "    records = []\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    total = len(combos)\n",
    "    if verbose:\n",
    "        print(f\"Starting fold-safe grid over {total} group combinations \"\n",
    "              f\"({n_splits}-fold CV).\")\n",
    "\n",
    "    for ci, include_groups in enumerate(combos, start=1):\n",
    "        fold_rmses = []\n",
    "        n_features_seen = None  # track from first fold\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n[{ci}/{total}] Groups: {include_groups}\")\n",
    "\n",
    "        for fold, (tr_idx, va_idx) in enumerate(kf.split(train), start=1):\n",
    "            tr_raw = train.iloc[tr_idx].copy()\n",
    "            va_raw = train.iloc[va_idx].copy()\n",
    "\n",
    "            # FE separately per fold to avoid leakage\n",
    "            tr_fe = create_engineered_features(tr_raw)\n",
    "            va_fe = create_engineered_features(va_raw)\n",
    "\n",
    "            y_tr = tr_fe[target_col].copy()\n",
    "            y_va = va_fe[target_col].copy()\n",
    "\n",
    "            # Drop target + ids to form feature matrices\n",
    "            drop_cols = [c for c in id_cols if c in tr_fe.columns] + [target_col]\n",
    "            X_tr_full = tr_fe.drop(columns=drop_cols, errors=\"ignore\")\n",
    "            X_va_full = va_fe.drop(columns=drop_cols, errors=\"ignore\")\n",
    "\n",
    "            # Select feature columns based on TRAIN-FOLD columns & requested groups\n",
    "            X_tr_sub = select_columns_by_groups(X_tr_full, include_groups, group_patterns)\n",
    "\n",
    "            if X_tr_sub.shape[1] == 0:\n",
    "                if verbose:\n",
    "                    print(\"  -> Skipping: no columns selected for this combo.\")\n",
    "                fold_rmses = []\n",
    "                break\n",
    "\n",
    "            # Align validation to selected columns\n",
    "            X_va_sub = X_va_full.reindex(columns=X_tr_sub.columns, fill_value=np.nan)\n",
    "\n",
    "            # Build & fit pipeline\n",
    "            pipe = build_lgbm_pipeline(X_tr_sub, random_state=random_state)\n",
    "            pipe.fit(X_tr_sub, y_tr)\n",
    "\n",
    "            pred = pipe.predict(X_va_sub)\n",
    "            rmse = mean_squared_error(y_va, pred)\n",
    "            fold_rmses.append(rmse)\n",
    "\n",
    "            if n_features_seen is None:\n",
    "                n_features_seen = X_tr_sub.shape[1]\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"  Fold {fold}/{n_splits}: RMSE = {rmse:.6f}\")\n",
    "\n",
    "        if not fold_rmses:\n",
    "            continue\n",
    "\n",
    "        mean_rmse = float(np.mean(fold_rmses))\n",
    "        std_rmse = float(np.std(fold_rmses))\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"  -> n_features: {n_features_seen}, \"\n",
    "                  f\"cv_rmse_mean: {mean_rmse:.6f}, cv_rmse_std: {std_rmse:.6f}\")\n",
    "\n",
    "        records.append({\n",
    "            \"groups\": include_groups,\n",
    "            \"n_features\": int(n_features_seen),\n",
    "            \"cv_rmse_mean\": mean_rmse,\n",
    "            \"cv_rmse_std\": std_rmse,\n",
    "            \"fold_rmses\": fold_rmses,\n",
    "        })\n",
    "\n",
    "        # Optional partial saves\n",
    "        if save_every and (ci % save_every == 0):\n",
    "            tmp = pd.DataFrame.from_records(records).sort_values(\"cv_rmse_mean\").reset_index(drop=True)\n",
    "            tmp.to_csv(save_path, index=False)\n",
    "            if verbose:\n",
    "                print(f\"  [autosave] Wrote partial results to {save_path} at combo {ci}/{total}\")\n",
    "\n",
    "    results = pd.DataFrame.from_records(records).sort_values(\"cv_rmse_mean\").reset_index(drop=True)\n",
    "    results.to_csv(save_path, index=False)\n",
    "    if verbose:\n",
    "        print(f\"\\nFinished. Saved fold-safe results to {save_path} with {len(results)} rows.\")\n",
    "    return results"
   ],
   "id": "24b1fec9b7843932",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## for Ã¥ hente ut data",
   "id": "94ca17a6691f4b92"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-29T01:17:53.202455Z",
     "start_time": "2025-10-29T01:17:53.198116Z"
    }
   },
   "source": [
    "def load_data():\n",
    "    # Read training, test, and sample submission datasets\n",
    "    train = pd.read_csv(\"input/train.csv\")\n",
    "    test = pd.read_csv(\"input/test.csv\")\n",
    "    sample_submission = pd.read_csv(\"input/sample_submission.csv\")\n",
    "\n",
    "    # Return all three datasets\n",
    "    return train, test, sample_submission\n"
   ],
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evt. feature engineering",
   "id": "c55cd82720e4d8c1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T01:17:53.654532Z",
     "start_time": "2025-10-29T01:17:53.647814Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_engineered_features(df):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame with raw features.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with added engineered features (non-destructive copy).\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Feature 1: speed / curvature (protect against div-by-zero with small epsilon)\n",
    "    speed_o_curve = df[\"speed_limit\"] / (df[\"curvature\"] + 1e-6)\n",
    "    df[\"speed_o_curve\"] = speed_o_curve.fillna(0)\n",
    "\n",
    "    # Feature 2 (disabled): speed squared\n",
    "    # speed_x_speed = df[\"speed_limit\"] ** 2\n",
    "    # df[\"speed_x_speed\"] = speed_x_speed.fillna(0)\n",
    "\n",
    "    # Feature 3 (disabled): speed * reported accidents\n",
    "    # speed_x_accidents = df[\"speed_limit\"] * df[\"num_reported_accidents\"]\n",
    "    # df[\"speed_x_accidents\"] = speed_x_accidents.fillna(0)\n",
    "\n",
    "    # Feature 4: visibility risk components (lighting + weather) and composite\n",
    "    lighting_w = {\"night\": 0.9, \"dim\": 0.3, \"daylight\": 0.1}\n",
    "    weather_w = {\"foggy\": 0.8, \"rainy\": 0.7, \"clear\": 0.1}\n",
    "\n",
    "    df[\"lighting_risk\"] = df[\"lighting\"].map(lighting_w).fillna(0)\n",
    "    df[\"weather_risk\"] = df[\"weather\"].map(weather_w).fillna(0)\n",
    "    df[\"visibility_composite\"] = (df[\"lighting_risk\"] + df[\"weather_risk\"]) / 2\n",
    "\n",
    "    # Feature 5: time of day as ordinal integer\n",
    "    time_order = {\"morning\": 1, \"evening\": 2, \"afternoon\": 3}\n",
    "    df[\"time_as_int\"] = df[\"time_of_day\"].map(time_order)\n",
    "\n",
    "    # Feature 6: log1p(speed / curvature)\n",
    "    df[\"log_speed_o_curve\"] = np.log1p(speed_o_curve)\n",
    "\n",
    "    # Feature 7: curvature * speed\n",
    "    df[\"curvature_x_speed\"] = df[\"curvature\"] * df[\"speed_limit\"]\n",
    "\n",
    "    # Feature 8: reported accidents per lane (add 1 to avoid div-by-zero)\n",
    "    df[\"accidents_o_lanes\"] = df[\"num_reported_accidents\"] / (df[\"num_lanes\"] + 1)\n",
    "\n",
    "    # Feature 9: speed * time (ordinal)\n",
    "    df[\"speed_time_interaction\"] = df[\"speed_limit\"] * df[\"time_as_int\"]\n",
    "\n",
    "    # Feature 10: curvature * time (ordinal)\n",
    "    df[\"curvature_time_interaction\"] = df[\"curvature\"] * df[\"time_as_int\"]\n",
    "\n",
    "    return df\n"
   ],
   "id": "423ebd91db598833",
   "outputs": [],
   "execution_count": 75
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## preparere features for bruk i modell",
   "id": "5cf06908492b44d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T01:17:54.356624Z",
     "start_time": "2025-10-29T01:17:54.350617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepare_features(train, test):\n",
    "    # Create engineered features\n",
    "    train = create_engineered_features(train)\n",
    "    test = create_engineered_features(test)\n",
    "\n",
    "    # Separate features and target variable\n",
    "    X = train.drop(columns=[\"accident_risk\", \"id\"])\n",
    "    y = train[\"accident_risk\"]\n",
    "\n",
    "    print(f\"Features: {X.shape[1]}\")\n",
    "    # Define categorical feature names\n",
    "    categorical_features = [\n",
    "        \"road_type\",\n",
    "        \"lighting\",\n",
    "        \"weather\",\n",
    "        \"time_of_day\"\n",
    "    ]\n",
    "\n",
    "    # Define numerical feature names\n",
    "    numerical_features = [\n",
    "        \"num_lanes\",                # base\n",
    "        \"curvature\",                # base\n",
    "        \"speed_limit\",              # base\n",
    "        \"num_reported_accidents\",   # base\n",
    "\n",
    "        \"speed_o_curve\",            # Feature 1\n",
    "        # \"speed_x_speed\",          # Feature 2 (disabled)\n",
    "        # \"speed_x_accidents\",      # Feature 3 (disabled)\n",
    "\n",
    "        \"visibility_composite\",     # Feature 4 (composite)\n",
    "        \"lighting_risk\",            # Feature 4 (component)\n",
    "        \"weather_risk\",             # Feature 4 (component)\n",
    "\n",
    "        \"time_as_int\",              # Feature 5\n",
    "        \"log_speed_o_curve\",        # Feature 6\n",
    "        \"curvature_x_speed\",        # Feature 7\n",
    "        \"accidents_o_lanes\",        # Feature 8\n",
    "        \"speed_time_interaction\",   # Feature 9\n",
    "        \"curvature_time_interaction\",  # Feature 10\n",
    "    ]\n",
    "\n",
    "\n",
    "    boolean_features = [\n",
    "        \"holiday\",\n",
    "        \"school_season\",\n",
    "        \"road_signs_present\",\n",
    "        \"public_road\"\n",
    "    ]\n",
    "\n",
    "        # --- Normalize column dtypes to avoid np.isnan / pd.NA type issues ---\n",
    "    for col in categorical_features:\n",
    "        X[col] = X[col].astype(\"string\").fillna(\"__MISSING__\")\n",
    "        test[col] = test[col].astype(\"string\").fillna(\"__MISSING__\")\n",
    "\n",
    "    for col in boolean_features:\n",
    "        X[col] = X[col].astype(\"boolean\")\n",
    "        test[col] = test[col].astype(\"boolean\")\n",
    "\n",
    "    for col in numerical_features:\n",
    "        # Coerce nullable numerics (like Int64) to float64\n",
    "        X[col] = pd.to_numeric(X[col], errors=\"coerce\")\n",
    "        test[col] = pd.to_numeric(test[col], errors=\"coerce\")\n",
    "\n",
    "    # --- Pipelines for each type ---\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True))\n",
    "    ])\n",
    "\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        # with_mean=False keeps it compatible with sparse output\n",
    "        (\"scaler\", StandardScaler(with_mean=False))\n",
    "    ])\n",
    "\n",
    "    # Combine everything in a ColumnTransformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", categorical_transformer, categorical_features),\n",
    "            (\"num\", numerical_transformer, numerical_features),\n",
    "            # You can include boolean features as numeric 0/1\n",
    "            (\"bool\", \"passthrough\", boolean_features)\n",
    "        ],\n",
    "        sparse_threshold=0.3\n",
    "    )\n",
    "\n",
    "    # Return features, target, test set, and preprocessor\n",
    "    return X, y, test, preprocessor"
   ],
   "id": "82855141887adfde",
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## bygge en modell (LGBM regressor i dette eksempelet",
   "id": "aaa9b8ca4b50e0be"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T01:17:55.708543Z",
     "start_time": "2025-10-29T01:17:55.703731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# Function: Build LightGBM model\n",
    "# ============================================================\n",
    "def build_lgbm_model(preprocessor):\n",
    "    # LightGBM hyperparameters\n",
    "    params = {\n",
    "        \"n_estimators\": 525,\n",
    "        \"learning_rate\": 0.06,\n",
    "        \"max_depth\": 8,\n",
    "        \"num_leaves\": 64,\n",
    "        \"subsample\": 0.8,\n",
    "        \"colsample_bytree\": 0.9,\n",
    "        \"reg_lambda\": 0.6,\n",
    "        \"reg_alpha\": 0.2\n",
    "    }\n",
    "\n",
    "    # Create pipeline with preprocessing and LightGBM\n",
    "    model = Pipeline(steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"regressor\", LGBMRegressor(\n",
    "            **params,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            verbosity=-1\n",
    "        ))\n",
    "    ])\n",
    "    return model"
   ],
   "id": "fbdbec6280b66678",
   "outputs": [],
   "execution_count": 77
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## skape submission fil",
   "id": "e293c8602154fb3a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T01:17:56.741360Z",
     "start_time": "2025-10-29T01:17:56.736444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# Function: Train models and create averaged submission\n",
    "# ============================================================\n",
    "def generate_submission(lgbm_model, xgb_model, X, y, test, sample_submission):\n",
    "    # Preprocess features\n",
    "    X_processed = lgbm_model.named_steps[\"preprocessor\"].fit_transform(X)\n",
    "    test_processed = lgbm_model.named_steps[\"preprocessor\"].transform(test)\n",
    "\n",
    "    # Train LightGBM\n",
    "    lgbm_model.named_steps[\"regressor\"].fit(\n",
    "        X_processed,\n",
    "        y,\n",
    "        eval_set=[(X_processed, y)],\n",
    "        eval_metric=\"rmse\",\n",
    "        callbacks=[\n",
    "            early_stopping(stopping_rounds=50),\n",
    "            log_evaluation(period=50)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "    # Generate predictions from both models\n",
    "    preds_lgbm = lgbm_model.named_steps[\"regressor\"].predict(test_processed)\n",
    "\n",
    "\n",
    "    # Prepare submission file\n",
    "    submission = sample_submission.copy()\n",
    "    # submission[\"accident_risk\"] = final_predictions\n",
    "    submission[\"accident_risk\"] = preds_lgbm\n",
    "\n",
    "    # Save CSV for Kaggle submission\n",
    "    submission.to_csv(\"submissions/22_engineered_features_basic_lgbm.csv\", index=False)\n",
    "\n",
    "\n",
    "    print(\"Submission file saved in submissions folder.\")"
   ],
   "id": "f28a3d1f060428a2",
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T01:17:57.427134Z",
     "start_time": "2025-10-29T01:17:57.423654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    # Load datasets\n",
    "    train, test, sample_submission = load_data()\n",
    "\n",
    "    # Prepare features and preprocessing\n",
    "    X, y, test, preprocessor = prepare_features(train, test)\n",
    "\n",
    "    # Build both models\n",
    "    lgbm_model = build_lgbm_model(preprocessor)\n",
    "    #xgb_model = build_xgb_model(preprocessor)\n",
    "\n",
    "    # Generate final averaged submission\n",
    "    generate_submission(lgbm_model, _, X, y, test, sample_submission)"
   ],
   "id": "bdeefe93ec87bc41",
   "outputs": [],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T00:14:19.884025Z",
     "start_time": "2025-10-29T00:14:08.316564Z"
    }
   },
   "cell_type": "code",
   "source": "main()",
   "id": "803e13be4bb52e15",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 22\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 0.0569068\ttraining's l2: 0.00323838\n",
      "[100]\ttraining's rmse: 0.0559971\ttraining's l2: 0.00313568\n",
      "[150]\ttraining's rmse: 0.0558546\ttraining's l2: 0.00311973\n",
      "[200]\ttraining's rmse: 0.0557457\ttraining's l2: 0.00310759\n",
      "[250]\ttraining's rmse: 0.0556473\ttraining's l2: 0.00309662\n",
      "[300]\ttraining's rmse: 0.0555626\ttraining's l2: 0.00308721\n",
      "[350]\ttraining's rmse: 0.0554857\ttraining's l2: 0.00307866\n",
      "[400]\ttraining's rmse: 0.0554078\ttraining's l2: 0.00307002\n",
      "[450]\ttraining's rmse: 0.0553369\ttraining's l2: 0.00306218\n",
      "[500]\ttraining's rmse: 0.0552726\ttraining's l2: 0.00305506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tebje\\Documents\\HVLIT\\GithubRepos\\AT-Road-Accident-Risk\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file saved in submissions folder.\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T00:58:21.570121Z",
     "start_time": "2025-10-29T00:58:12.196593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train, test, _ = load_data()\n",
    "\n",
    "# ðŸ‘‰ Customize these patterns to match your column names if needed\n",
    "group_patterns = group_patterns  # or edit a copy\n",
    "\n",
    "results = run_feature_set_grid_search(\n",
    "    train=train,\n",
    "    target_col=\"accident_risk\",     # adjust if your target name differs\n",
    "    group_patterns=group_patterns,\n",
    "    id_cols=[\"id\"],                 # add any extra ID-like cols to drop\n",
    "    n_splits=5,\n",
    "    random_state=42,\n",
    "    max_groups=None,                # or set to a small int to limit combo size\n",
    "    save_path=\"feature_grid_results.csv\"\n",
    ")\n",
    "\n",
    "results.head(20)"
   ],
   "id": "7d3c7c28c52d869b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tebje\\Documents\\HVLIT\\GithubRepos\\AT-Road-Accident-Risk\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[43]\u001B[39m\u001B[32m, line 6\u001B[39m\n\u001B[32m      3\u001B[39m \u001B[38;5;66;03m# ðŸ‘‰ Customize these patterns to match your column names if needed\u001B[39;00m\n\u001B[32m      4\u001B[39m group_patterns = group_patterns  \u001B[38;5;66;03m# or edit a copy\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m results = \u001B[43mrun_feature_set_grid_search\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtarget_col\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43maccident_risk\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m     \u001B[49m\u001B[38;5;66;43;03m# adjust if your target name differs\u001B[39;49;00m\n\u001B[32m      9\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgroup_patterns\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup_patterns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     10\u001B[39m \u001B[43m    \u001B[49m\u001B[43mid_cols\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mid\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m                 \u001B[49m\u001B[38;5;66;43;03m# add any extra ID-like cols to drop\u001B[39;49;00m\n\u001B[32m     11\u001B[39m \u001B[43m    \u001B[49m\u001B[43mn_splits\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m5\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     12\u001B[39m \u001B[43m    \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m42\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     13\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmax_groups\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m                \u001B[49m\u001B[38;5;66;43;03m# or set to a small int to limit combo size\u001B[39;49;00m\n\u001B[32m     14\u001B[39m \u001B[43m    \u001B[49m\u001B[43msave_path\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mfeature_grid_results.csv\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\n\u001B[32m     15\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m     17\u001B[39m results.head(\u001B[32m20\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[37]\u001B[39m\u001B[32m, line 177\u001B[39m, in \u001B[36mrun_feature_set_grid_search\u001B[39m\u001B[34m(train, target_col, group_patterns, id_cols, n_splits, random_state, max_groups, save_path)\u001B[39m\n\u001B[32m    174\u001B[39m y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n\u001B[32m    176\u001B[39m pipe = build_lgbm_pipeline(X_tr, random_state=random_state)\n\u001B[32m--> \u001B[39m\u001B[32m177\u001B[39m \u001B[43mpipe\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_tr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_tr\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    179\u001B[39m pred = pipe.predict(X_va)\n\u001B[32m    180\u001B[39m rmse = mean_squared_error(y_va, pred)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\HVLIT\\GithubRepos\\AT-Road-Accident-Risk\\.venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001B[39m, in \u001B[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(estimator, *args, **kwargs)\u001B[39m\n\u001B[32m   1358\u001B[39m     estimator._validate_params()\n\u001B[32m   1360\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[32m   1361\u001B[39m     skip_parameter_validation=(\n\u001B[32m   1362\u001B[39m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[32m   1363\u001B[39m     )\n\u001B[32m   1364\u001B[39m ):\n\u001B[32m-> \u001B[39m\u001B[32m1365\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\HVLIT\\GithubRepos\\AT-Road-Accident-Risk\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:663\u001B[39m, in \u001B[36mPipeline.fit\u001B[39m\u001B[34m(self, X, y, **params)\u001B[39m\n\u001B[32m    657\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._final_estimator != \u001B[33m\"\u001B[39m\u001B[33mpassthrough\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    658\u001B[39m         last_step_params = \u001B[38;5;28mself\u001B[39m._get_metadata_for_step(\n\u001B[32m    659\u001B[39m             step_idx=\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m) - \u001B[32m1\u001B[39m,\n\u001B[32m    660\u001B[39m             step_params=routed_params[\u001B[38;5;28mself\u001B[39m.steps[-\u001B[32m1\u001B[39m][\u001B[32m0\u001B[39m]],\n\u001B[32m    661\u001B[39m             all_params=params,\n\u001B[32m    662\u001B[39m         )\n\u001B[32m--> \u001B[39m\u001B[32m663\u001B[39m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_final_estimator\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mXt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mlast_step_params\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mfit\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    665\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\HVLIT\\GithubRepos\\AT-Road-Accident-Risk\\.venv\\Lib\\site-packages\\lightgbm\\sklearn.py:1398\u001B[39m, in \u001B[36mLGBMRegressor.fit\u001B[39m\u001B[34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001B[39m\n\u001B[32m   1381\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mfit\u001B[39m(  \u001B[38;5;66;03m# type: ignore[override]\u001B[39;00m\n\u001B[32m   1382\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   1383\u001B[39m     X: _LGBM_ScikitMatrixLike,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1395\u001B[39m     init_model: Optional[Union[\u001B[38;5;28mstr\u001B[39m, Path, Booster, LGBMModel]] = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1396\u001B[39m ) -> \u001B[33m\"\u001B[39m\u001B[33mLGBMRegressor\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m   1397\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1398\u001B[39m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1399\u001B[39m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1400\u001B[39m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1401\u001B[39m \u001B[43m        \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m=\u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1402\u001B[39m \u001B[43m        \u001B[49m\u001B[43minit_score\u001B[49m\u001B[43m=\u001B[49m\u001B[43minit_score\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1403\u001B[39m \u001B[43m        \u001B[49m\u001B[43meval_set\u001B[49m\u001B[43m=\u001B[49m\u001B[43meval_set\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1404\u001B[39m \u001B[43m        \u001B[49m\u001B[43meval_names\u001B[49m\u001B[43m=\u001B[49m\u001B[43meval_names\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1405\u001B[39m \u001B[43m        \u001B[49m\u001B[43meval_sample_weight\u001B[49m\u001B[43m=\u001B[49m\u001B[43meval_sample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1406\u001B[39m \u001B[43m        \u001B[49m\u001B[43meval_init_score\u001B[49m\u001B[43m=\u001B[49m\u001B[43meval_init_score\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1407\u001B[39m \u001B[43m        \u001B[49m\u001B[43meval_metric\u001B[49m\u001B[43m=\u001B[49m\u001B[43meval_metric\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1408\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfeature_name\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfeature_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1409\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcategorical_feature\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcategorical_feature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1410\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1411\u001B[39m \u001B[43m        \u001B[49m\u001B[43minit_model\u001B[49m\u001B[43m=\u001B[49m\u001B[43minit_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1412\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1413\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\HVLIT\\GithubRepos\\AT-Road-Accident-Risk\\.venv\\Lib\\site-packages\\lightgbm\\sklearn.py:1049\u001B[39m, in \u001B[36mLGBMModel.fit\u001B[39m\u001B[34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001B[39m\n\u001B[32m   1046\u001B[39m evals_result: _EvalResultDict = {}\n\u001B[32m   1047\u001B[39m callbacks.append(record_evaluation(evals_result))\n\u001B[32m-> \u001B[39m\u001B[32m1049\u001B[39m \u001B[38;5;28mself\u001B[39m._Booster = \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1050\u001B[39m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m=\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1051\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrain_set\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrain_set\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1052\u001B[39m \u001B[43m    \u001B[49m\u001B[43mnum_boost_round\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mn_estimators\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1053\u001B[39m \u001B[43m    \u001B[49m\u001B[43mvalid_sets\u001B[49m\u001B[43m=\u001B[49m\u001B[43mvalid_sets\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1054\u001B[39m \u001B[43m    \u001B[49m\u001B[43mvalid_names\u001B[49m\u001B[43m=\u001B[49m\u001B[43meval_names\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1055\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfeval\u001B[49m\u001B[43m=\u001B[49m\u001B[43meval_metrics_callable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[32m   1056\u001B[39m \u001B[43m    \u001B[49m\u001B[43minit_model\u001B[49m\u001B[43m=\u001B[49m\u001B[43minit_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1057\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1058\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1060\u001B[39m \u001B[38;5;66;03m# This populates the property self.n_features_, the number of features in the fitted model,\u001B[39;00m\n\u001B[32m   1061\u001B[39m \u001B[38;5;66;03m# and so should only be set after fitting.\u001B[39;00m\n\u001B[32m   1062\u001B[39m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[32m   1063\u001B[39m \u001B[38;5;66;03m# The related property self._n_features_in, which populates self.n_features_in_,\u001B[39;00m\n\u001B[32m   1064\u001B[39m \u001B[38;5;66;03m# is set BEFORE fitting.\u001B[39;00m\n\u001B[32m   1065\u001B[39m \u001B[38;5;28mself\u001B[39m._n_features = \u001B[38;5;28mself\u001B[39m._Booster.num_feature()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\HVLIT\\GithubRepos\\AT-Road-Accident-Risk\\.venv\\Lib\\site-packages\\lightgbm\\engine.py:322\u001B[39m, in \u001B[36mtrain\u001B[39m\u001B[34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, keep_training_booster, callbacks)\u001B[39m\n\u001B[32m    310\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m cb \u001B[38;5;129;01min\u001B[39;00m callbacks_before_iter:\n\u001B[32m    311\u001B[39m     cb(\n\u001B[32m    312\u001B[39m         callback.CallbackEnv(\n\u001B[32m    313\u001B[39m             model=booster,\n\u001B[32m   (...)\u001B[39m\u001B[32m    319\u001B[39m         )\n\u001B[32m    320\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m322\u001B[39m \u001B[43mbooster\u001B[49m\u001B[43m.\u001B[49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfobj\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    324\u001B[39m evaluation_result_list: List[_LGBM_BoosterEvalMethodResultType] = []\n\u001B[32m    325\u001B[39m \u001B[38;5;66;03m# check evaluation result.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\HVLIT\\GithubRepos\\AT-Road-Accident-Risk\\.venv\\Lib\\site-packages\\lightgbm\\basic.py:4155\u001B[39m, in \u001B[36mBooster.update\u001B[39m\u001B[34m(self, train_set, fobj)\u001B[39m\n\u001B[32m   4152\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.__set_objective_to_none:\n\u001B[32m   4153\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m LightGBMError(\u001B[33m\"\u001B[39m\u001B[33mCannot update due to null objective function.\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m   4154\u001B[39m _safe_call(\n\u001B[32m-> \u001B[39m\u001B[32m4155\u001B[39m     \u001B[43m_LIB\u001B[49m\u001B[43m.\u001B[49m\u001B[43mLGBM_BoosterUpdateOneIter\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   4156\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4157\u001B[39m \u001B[43m        \u001B[49m\u001B[43mctypes\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbyref\u001B[49m\u001B[43m(\u001B[49m\u001B[43mis_finished\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4158\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   4159\u001B[39m )\n\u001B[32m   4160\u001B[39m \u001B[38;5;28mself\u001B[39m.__is_predicted_cur_iter = [\u001B[38;5;28;01mFalse\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m.__num_dataset)]\n\u001B[32m   4161\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m is_finished.value == \u001B[32m1\u001B[39m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T01:18:45.815177Z",
     "start_time": "2025-10-29T01:18:40.178934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "safe_results = run_feature_set_grid_search_fold_safe(\n",
    "    train=train,\n",
    "    target_col=\"accident_risk\",\n",
    "    group_patterns=group_patterns,   # your patterns dict\n",
    "    id_cols=[\"id\"],\n",
    "    n_splits=5,\n",
    "    random_state=42,\n",
    "    max_groups=None,\n",
    "    save_path=\"feature_grid_results_foldsafe.csv\",\n",
    ")\n",
    "\n",
    "safe_results.head(20)"
   ],
   "id": "73f7370b68e00151",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fold-safe grid over 255 group combinations (5-fold CV).\n",
      "\n",
      "[1/255] Groups: ['categorical']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tebje\\Documents\\HVLIT\\GithubRepos\\AT-Road-Accident-Risk\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'rmse' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mUnboundLocalError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[83]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m safe_results = \u001B[43mrun_feature_set_grid_search_fold_safe\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      2\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      3\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtarget_col\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43maccident_risk\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgroup_patterns\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup_patterns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m   \u001B[49m\u001B[38;5;66;43;03m# your patterns dict\u001B[39;49;00m\n\u001B[32m      5\u001B[39m \u001B[43m    \u001B[49m\u001B[43mid_cols\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mid\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m    \u001B[49m\u001B[43mn_splits\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m5\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[43m    \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m42\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmax_groups\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m      9\u001B[39m \u001B[43m    \u001B[49m\u001B[43msave_path\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mfeature_grid_results_foldsafe.csv\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     10\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m     12\u001B[39m safe_results.head(\u001B[32m20\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[73]\u001B[39m\u001B[32m, line 90\u001B[39m, in \u001B[36mrun_feature_set_grid_search_fold_safe\u001B[39m\u001B[34m(train, target_col, group_patterns, id_cols, n_splits, random_state, max_groups, save_path, verbose, save_every)\u001B[39m\n\u001B[32m     87\u001B[39m pipe.fit(X_tr_sub, y_tr)\n\u001B[32m     89\u001B[39m pred = pipe.predict(X_va_sub)\n\u001B[32m---> \u001B[39m\u001B[32m90\u001B[39m rmse = \u001B[43mrmse\u001B[49m(y_va, pred)\n\u001B[32m     91\u001B[39m fold_rmses.append(rmse)\n\u001B[32m     93\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m n_features_seen \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[31mUnboundLocalError\u001B[39m: cannot access local variable 'rmse' where it is not associated with a value"
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T01:18:35.742211Z",
     "start_time": "2025-10-29T01:18:28.805964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "safe_only_patterns = {\n",
    "    \"categorical\": [r\"^(road_type|lighting|weather|time_of_day)$\"],\n",
    "    \"booleans\":    [r\"^(holiday|school_season|road_signs_present|public_road)$\"],\n",
    "    \"base_numeric\":[r\"^(num_lanes|curvature|speed_limit|num_reported_accidents)$\"],\n",
    "}\n",
    "\n",
    "safe_only_results = run_feature_set_grid_search_fold_safe(\n",
    "    train=train,\n",
    "    target_col=\"accident_risk\",\n",
    "    group_patterns=safe_only_patterns,\n",
    "    id_cols=[\"id\"],\n",
    "    n_splits=5,\n",
    "    random_state=42,\n",
    "    verbose=True\n",
    ")\n",
    "safe_only_results.head()"
   ],
   "id": "24332ec4db42523e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fold-safe grid over 7 group combinations (5-fold CV).\n",
      "\n",
      "[1/7] Groups: ['categorical']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tebje\\Documents\\HVLIT\\GithubRepos\\AT-Road-Accident-Risk\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'rmse' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mUnboundLocalError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[82]\u001B[39m\u001B[32m, line 7\u001B[39m\n\u001B[32m      1\u001B[39m safe_only_patterns = {\n\u001B[32m      2\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mcategorical\u001B[39m\u001B[33m\"\u001B[39m: [\u001B[33mr\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m^(road_type|lighting|weather|time_of_day)$\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m      3\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mbooleans\u001B[39m\u001B[33m\"\u001B[39m:    [\u001B[33mr\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m^(holiday|school_season|road_signs_present|public_road)$\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m      4\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mbase_numeric\u001B[39m\u001B[33m\"\u001B[39m:[\u001B[33mr\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m^(num_lanes|curvature|speed_limit|num_reported_accidents)$\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m      5\u001B[39m }\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m safe_only_results = \u001B[43mrun_feature_set_grid_search_fold_safe\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      9\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtarget_col\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43maccident_risk\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     10\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgroup_patterns\u001B[49m\u001B[43m=\u001B[49m\u001B[43msafe_only_patterns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     11\u001B[39m \u001B[43m    \u001B[49m\u001B[43mid_cols\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mid\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     12\u001B[39m \u001B[43m    \u001B[49m\u001B[43mn_splits\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m5\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     13\u001B[39m \u001B[43m    \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m42\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     14\u001B[39m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\n\u001B[32m     15\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m     16\u001B[39m safe_only_results.head()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[73]\u001B[39m\u001B[32m, line 90\u001B[39m, in \u001B[36mrun_feature_set_grid_search_fold_safe\u001B[39m\u001B[34m(train, target_col, group_patterns, id_cols, n_splits, random_state, max_groups, save_path, verbose, save_every)\u001B[39m\n\u001B[32m     87\u001B[39m pipe.fit(X_tr_sub, y_tr)\n\u001B[32m     89\u001B[39m pred = pipe.predict(X_va_sub)\n\u001B[32m---> \u001B[39m\u001B[32m90\u001B[39m rmse = \u001B[43mrmse\u001B[49m(y_va, pred)\n\u001B[32m     91\u001B[39m fold_rmses.append(rmse)\n\u001B[32m     93\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m n_features_seen \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[31mUnboundLocalError\u001B[39m: cannot access local variable 'rmse' where it is not associated with a value"
     ]
    }
   ],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T01:19:33.870703Z",
     "start_time": "2025-10-29T01:19:33.856768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def compute_rmse(y_true, y_pred) -> float:\n",
    "    return float(np.sqrt(mse(y_true, y_pred)))\n",
    "\n",
    "\n",
    "def run_feature_set_grid_search_replicate_main(\n",
    "    train, test, target_col, group_patterns, id_cols=None,\n",
    "    n_splits=5, random_state=42, max_groups=None,\n",
    "    save_path=\"feature_grid_results_replicate_main.csv\",\n",
    "    verbose=True, save_every=0,\n",
    "):\n",
    "    id_cols = id_cols or [\"id\"]\n",
    "\n",
    "    # Prepare once exactly like main()\n",
    "    X, y, test_processed, preprocessor = prepare_features(train.copy(), test.copy())\n",
    "    for c in id_cols:\n",
    "        if c in X.columns:\n",
    "            X = X.drop(columns=[c])\n",
    "\n",
    "    combos = expand_boolean_grid(list(group_patterns.keys()), max_groups=max_groups)\n",
    "    if not combos:\n",
    "        raise RuntimeError(\"No group combinations produced. Check your group_patterns.\")\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    records = []\n",
    "    total = len(combos)\n",
    "    if verbose:\n",
    "        print(f\"Starting grid over {total} combos ({n_splits}-fold CV) using your main() preprocessing.\")\n",
    "\n",
    "    for ci, include_groups in enumerate(combos, start=1):\n",
    "        if verbose:\n",
    "            print(f\"\\n[{ci}/{total}] Groups: {include_groups}\")\n",
    "\n",
    "        X_sub = select_columns_by_groups(X, include_groups, group_patterns)\n",
    "        if X_sub.shape[1] == 0:\n",
    "            if verbose:\n",
    "                print(\"  -> Skipping: no columns selected for this combo.\")\n",
    "            continue\n",
    "\n",
    "        fold_rmses = []\n",
    "        n_features_seen = X_sub.shape[1]\n",
    "\n",
    "        for fold, (tr_idx, va_idx) in enumerate(kf.split(X_sub), start=1):\n",
    "            X_tr, X_va = X_sub.iloc[tr_idx], X_sub.iloc[va_idx]\n",
    "            y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "\n",
    "            # Trim the cloned preprocessor to match current columns\n",
    "            pp_trim = trim_preprocessor_columns(preprocessor, X_tr.columns)\n",
    "            model = build_lgbm_model(pp_trim)\n",
    "\n",
    "            model.fit(X_tr, y_tr)\n",
    "            pred = model.predict(X_va)\n",
    "            rmse_val = compute_rmse(y_va, pred)   # <-- use compute_rmse\n",
    "            fold_rmses.append(rmse_val)\n",
    "            \n",
    "            if verbose:\n",
    "                if fold == 1:\n",
    "                    null_rmse = compute_rmse(y_va, np.full_like(y_va, y_tr.mean()))\n",
    "                    print(f\"  Fold 1 null baseline RMSE = {null_rmse:.6f}\")\n",
    "                print(f\"  Fold {fold}/{n_splits}: RMSE = {rmse_val:.6f}\")\n",
    "\n",
    "        mean_rmse = float(np.mean(fold_rmses))\n",
    "        std_rmse  = float(np.std(fold_rmses))\n",
    "        if verbose:\n",
    "            print(f\"  -> n_features: {n_features_seen}, \"\n",
    "                  f\"cv_rmse_mean: {mean_rmse:.6f}, cv_rmse_std: {std_rmse:.6f}\")\n",
    "\n",
    "        records.append({\n",
    "            \"groups\": include_groups,\n",
    "            \"n_features\": int(n_features_seen),\n",
    "            \"cv_rmse_mean\": mean_rmse,\n",
    "            \"cv_rmse_std\": std_rmse,\n",
    "            \"fold_rmses\": fold_rmses,\n",
    "        })\n",
    "\n",
    "        if save_every and (ci % save_every == 0):\n",
    "            tmp = pd.DataFrame.from_records(records).sort_values(\"cv_rmse_mean\").reset_index(drop=True)\n",
    "            tmp.to_csv(save_path, index=False)\n",
    "            if verbose:\n",
    "                print(f\"  [autosave] Wrote partial results to {save_path} at combo {ci}/{total}\")\n",
    "\n",
    "    results = pd.DataFrame.from_records(records).sort_values(\"cv_rmse_mean\").reset_index(drop=True)\n",
    "    results.to_csv(save_path, index=False)\n",
    "    if verbose:\n",
    "        print(f\"\\nFinished. Saved results to {save_path} with {len(results)} rows.\")\n",
    "    return results\n"
   ],
   "id": "b0135d56e0a2339b",
   "outputs": [],
   "execution_count": 84
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-10-29T01:19:34.202094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rep_results = run_feature_set_grid_search_replicate_main(\n",
    "    train=train,\n",
    "    test=test,\n",
    "    target_col=\"accident_risk\",\n",
    "    group_patterns=group_patterns,\n",
    "    id_cols=[\"id\"],\n",
    "    n_splits=5,\n",
    "    random_state=42,\n",
    "    max_groups=None,\n",
    "    save_path=\"feature_grid_results_replicate_main.csv\",\n",
    "    verbose=True,\n",
    "    save_every=10,\n",
    ")\n",
    "rep_results.head(20)\n"
   ],
   "id": "87b3f7da5b03e359",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 22\n",
      "Starting grid over 255 combos (5-fold CV) using your main() preprocessing.\n",
      "\n",
      "[1/255] Groups: ['categorical']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tebje\\Documents\\HVLIT\\GithubRepos\\AT-Road-Accident-Risk\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 1 null baseline RMSE = 0.166173\n",
      "  Fold 1/5: RMSE = 0.142244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tebje\\Documents\\HVLIT\\GithubRepos\\AT-Road-Accident-Risk\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 2/5: RMSE = 0.142356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tebje\\Documents\\HVLIT\\GithubRepos\\AT-Road-Accident-Risk\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 3/5: RMSE = 0.142692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tebje\\Documents\\HVLIT\\GithubRepos\\AT-Road-Accident-Risk\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 4/5: RMSE = 0.142181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tebje\\Documents\\HVLIT\\GithubRepos\\AT-Road-Accident-Risk\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 5/5: RMSE = 0.142438\n",
      "  -> n_features: 4, cv_rmse_mean: 0.142382, cv_rmse_std: 0.000179\n",
      "\n",
      "[2/255] Groups: ['booleans']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tebje\\Documents\\HVLIT\\GithubRepos\\AT-Road-Accident-Risk\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 1 null baseline RMSE = 0.166173\n",
      "  Fold 1/5: RMSE = 0.165856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tebje\\Documents\\HVLIT\\GithubRepos\\AT-Road-Accident-Risk\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 2/5: RMSE = 0.166176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tebje\\Documents\\HVLIT\\GithubRepos\\AT-Road-Accident-Risk\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 3/5: RMSE = 0.166567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tebje\\Documents\\HVLIT\\GithubRepos\\AT-Road-Accident-Risk\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 4/5: RMSE = 0.165705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tebje\\Documents\\HVLIT\\GithubRepos\\AT-Road-Accident-Risk\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 5/5: RMSE = 0.166269\n",
      "  -> n_features: 4, cv_rmse_mean: 0.166114, cv_rmse_std: 0.000306\n",
      "\n",
      "[3/255] Groups: ['base_numeric']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tebje\\Documents\\HVLIT\\GithubRepos\\AT-Road-Accident-Risk\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 1 null baseline RMSE = 0.166173\n",
      "  Fold 1/5: RMSE = 0.105991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tebje\\Documents\\HVLIT\\GithubRepos\\AT-Road-Accident-Risk\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 2/5: RMSE = 0.106364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tebje\\Documents\\HVLIT\\GithubRepos\\AT-Road-Accident-Risk\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 3/5: RMSE = 0.106357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tebje\\Documents\\HVLIT\\GithubRepos\\AT-Road-Accident-Risk\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 4/5: RMSE = 0.105707\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5751be2557ea2380"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
