{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# LGBM regressor\n",
    "LGM regressor scorer ofte bra, jeg har valgt å bruke denne for å se hva vi kan få ut av datasettet uten større mengde feature engineering.\n",
    "\n",
    "## importere bibloteker"
   ],
   "id": "b4ff438af74554d3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "dca1603cf68a68fe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T12:05:07.768058Z",
     "start_time": "2025-11-01T12:05:07.716053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "import joblib, os\n"
   ],
   "id": "f0ca7c9fcd13ccc7",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## for å hente ut data",
   "id": "94ca17a6691f4b92"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-01T12:05:08.500492Z",
     "start_time": "2025-11-01T12:05:08.493513Z"
    }
   },
   "source": [
    "def load_data():\n",
    "    # Read training, test, and sample submission datasets\n",
    "    train = pd.read_csv(\"input/train.csv\")\n",
    "    test = pd.read_csv(\"input/test.csv\")\n",
    "    sample_submission = pd.read_csv(\"input/sample_submission.csv\")\n",
    "\n",
    "    # Return all three datasets\n",
    "    return train, test, sample_submission\n"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evt. feature engineering",
   "id": "c55cd82720e4d8c1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T12:05:09.809880Z",
     "start_time": "2025-11-01T12:05:09.775755Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_engineered_features(df):\n",
    "    \"\"\"\n",
    "    These are all the parameters that ended up being used in the final model\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Feature 1: speed / curvature (protect against div-by-zero with small epsilon)\n",
    "    speed_o_curve = df[\"speed_limit\"] / (df[\"curvature\"] + 1e-6)\n",
    "    df[\"speed_o_curve\"] = speed_o_curve.fillna(0)\n",
    "\n",
    "    # Feature 4: visibility risk components (lighting + weather) and composite\n",
    "    lighting_w = {\"night\": 0.9, \"dim\": 0.3, \"daylight\": 0.1}\n",
    "    weather_w = {\"foggy\": 0.8, \"rainy\": 0.7, \"clear\": 0.1}\n",
    "\n",
    "    df[\"lighting_risk\"] = df[\"lighting\"].map(lighting_w).fillna(0)\n",
    "    df[\"weather_risk\"] = df[\"weather\"].map(weather_w).fillna(0)\n",
    "    df[\"visibility_composite\"] = (df[\"lighting_risk\"] + df[\"weather_risk\"]) / 2\n",
    "\n",
    "    # Feature 5: time of day as ordinal integer\n",
    "    time_order = {\"morning\": 1, \"evening\": 2, \"afternoon\": 3}\n",
    "    df[\"time_as_int\"] = df[\"time_of_day\"].map(time_order)\n",
    "\n",
    "    return df"
   ],
   "id": "423ebd91db598833",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## preparere features for bruk i modell",
   "id": "5cf06908492b44d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T12:05:11.327910Z",
     "start_time": "2025-11-01T12:05:11.295245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepare_features(train, test):\n",
    "    # Create engineered features\n",
    "    train = create_engineered_features(train)\n",
    "    test = create_engineered_features(test)\n",
    "\n",
    "    # Separate features and target variable\n",
    "    X = train.drop(columns=[\"accident_risk\", \"id\"])\n",
    "    y = train[\"accident_risk\"]\n",
    "\n",
    "\n",
    "    # Define categorical feature names\n",
    "    categorical_features = [\n",
    "        \"road_type\",\n",
    "        \"lighting\",\n",
    "        \"weather\",\n",
    "        \"time_of_day\"\n",
    "    ]\n",
    "\n",
    "    # Define numerical feature names\n",
    "    numerical_features = [\n",
    "        \"num_lanes\",                # base\n",
    "        \"curvature\",                # base\n",
    "        \"speed_limit\",              # base\n",
    "        \"num_reported_accidents\",   # base\n",
    "        \"speed_o_curve\",            # Feature 1\n",
    "        \"visibility_composite\",     # Feature 4 (composite)\n",
    "        \"lighting_risk\",            # Feature 4 (component)\n",
    "        \"weather_risk\",             # Feature 4 (component)\n",
    "        \"time_as_int\",              # Feature 5\n",
    "    ]\n",
    "\n",
    "\n",
    "    boolean_features = [\n",
    "        \"holiday\",\n",
    "        \"school_season\",\n",
    "        \"road_signs_present\",\n",
    "        \"public_road\"\n",
    "    ]\n",
    "\n",
    "    print(f\"Features: {X.shape[1]}\")\n",
    "\n",
    "        # --- Normalize column dtypes to avoid np.isnan / pd.NA type issues ---\n",
    "    for col in categorical_features:\n",
    "        if col in X.columns:\n",
    "            X[col] = X[col].astype(\"string\").fillna(\"__MISSING__\")\n",
    "            test[col] = test[col].astype(\"string\").fillna(\"__MISSING__\")\n",
    "\n",
    "\n",
    "    for col in boolean_features:\n",
    "        X[col] = X[col].astype(\"boolean\")\n",
    "        test[col] = test[col].astype(\"boolean\")\n",
    "\n",
    "    for col in numerical_features:\n",
    "        # Coerce nullable numerics (like Int64) to float64\n",
    "        X[col] = pd.to_numeric(X[col], errors=\"coerce\")\n",
    "        test[col] = pd.to_numeric(test[col], errors=\"coerce\")\n",
    "\n",
    "    # --- Pipelines for each type ---\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True))\n",
    "    ])\n",
    "\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        # with_mean=False keeps it compatible with sparse output\n",
    "        (\"scaler\", StandardScaler(with_mean=False))\n",
    "    ])\n",
    "\n",
    "    # Combine everything in a ColumnTransformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            #(\"cat\", categorical_transformer, categorical_features),\n",
    "            (\"num\", numerical_transformer, numerical_features),\n",
    "            # You can include boolean features as numeric 0/1\n",
    "            (\"bool\", \"passthrough\", boolean_features)\n",
    "        ],\n",
    "        sparse_threshold=0.3\n",
    "    )\n",
    "\n",
    "    # Return features, target, test set, and preprocessor\n",
    "    return X, y, test, preprocessor"
   ],
   "id": "82855141887adfde",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Stacked bodel lgbm, rf, enet",
   "id": "734ab36dd8840d66"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T12:05:13.210508Z",
     "start_time": "2025-11-01T12:05:13.191316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_lgbm_best():\n",
    "    return LGBMRegressor(\n",
    "        n_estimators=1542,\n",
    "        learning_rate=0.004624613524705627,\n",
    "        max_depth=12,\n",
    "        num_leaves=258,\n",
    "        subsample=0.6963927503033583,\n",
    "        colsample_bytree=0.9755243540395523,\n",
    "        reg_lambda=0.0034553945666010275,\n",
    "        reg_alpha=0.12863137655092372,\n",
    "        min_child_samples=8,\n",
    "        subsample_freq=1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbosity=-1\n",
    "    )\n",
    "\n",
    "def build_lgbm_model(preprocessor):\n",
    "    print(\"Building LightGBM model...\")\n",
    "    return Pipeline(steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"regressor\", get_lgbm_best())\n",
    "    ])\n",
    "\n",
    "def build_stacked_model(preprocessor):\n",
    "    print(\"Building Stacked model...\")\n",
    "    lgbm = get_lgbm_best()\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=200,        # cut trees\n",
    "        max_depth=16,            # cap depth\n",
    "        min_samples_leaf=5,      # fewer leaves\n",
    "        max_features=\"sqrt\",\n",
    "        n_jobs=1,                # <<< 1\n",
    "        random_state=42\n",
    "    )\n",
    "    print(\"Building Stacked model...\")\n",
    "    enet = ElasticNet(alpha=0.0005, l1_ratio=0.1, max_iter=2000, random_state=42)\n",
    "\n",
    "    print(\"Building Stacked model...\")\n",
    "    stack = StackingRegressor(\n",
    "        estimators=[(\"lgbm\", lgbm), (\"rf\", rf), (\"enet\", enet)],\n",
    "        final_estimator=RidgeCV(alphas=np.logspace(-4, 4, 25)),\n",
    "        cv=KFold(n_splits=5, shuffle=True, random_state=42),\n",
    "        passthrough=False,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    return Pipeline(steps=[(\"preprocessor\", preprocessor), (\"stack\", stack)])\n",
    "\n",
    "def evaluate_cv(model, X, y, scoring=\"neg_root_mean_squared_error\"):\n",
    "    print(\"Evaluating CV...\")\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "    mean, std = np.mean(scores), np.std(scores)\n",
    "    return mean, std, scores\n"
   ],
   "id": "da6e621db412c107",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## skape submission fil",
   "id": "e293c8602154fb3a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T12:13:09.300987Z",
     "start_time": "2025-11-01T12:13:09.296460Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_submission(model, test, sample_submission, path=\"/sumbissions/final_stack.csv\"):\n",
    "    # Predict with the fitted pipeline\n",
    "    preds = model.predict(test)\n",
    "\n",
    "    # Prepare submission\n",
    "    submission = sample_submission.copy()\n",
    "    submission[\"accident_risk\"] = preds\n",
    "\n",
    "    # Ensure folder exists and save\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    submission.to_csv(path, index=False)\n",
    "    print(f\"Submission saved to {path}\")\n"
   ],
   "id": "f28a3d1f060428a2",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T12:05:15.907677Z",
     "start_time": "2025-11-01T12:05:15.901611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    # Load datasets\n",
    "    train, test, sample_submission = load_data()\n",
    "\n",
    "    # Prepare features and preprocessing\n",
    "    X, y, test, preprocessor = prepare_features(train, test)\n",
    "\n",
    "    # Build candidate models\n",
    "    stacked_model = build_stacked_model(preprocessor)\n",
    "\n",
    "    # Fit once here\n",
    "    stacked_model.fit(X, y)\n",
    "\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    joblib.dump(stacked_model, \"models/road_risk_lgbm_stacked.joblib\")\n",
    "    print(\"Model saved to models/road_risk_lgbm_stacked.joblib\")\n",
    "\n",
    "    # Make submission (no re-fit)\n",
    "    generate_submission(stacked_model, test, sample_submission)"
   ],
   "id": "bdeefe93ec87bc41",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T12:11:17.804549Z",
     "start_time": "2025-11-01T12:05:16.673830Z"
    }
   },
   "cell_type": "code",
   "source": "main()",
   "id": "803e13be4bb52e15",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 17\n",
      "Building Stacked model...\n",
      "Building Stacked model...\n",
      "Building Stacked model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tebje\\Documents\\HVLIT\\GithubRepos\\AT-Road-Accident-Risk\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved to /kaggle/working/submission.csv\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T12:12:16.367748Z",
     "start_time": "2025-11-01T12:12:16.289886Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "7d3c7c28c52d869b",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stacked_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[43]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[43mstacked_model\u001B[49m)\n",
      "\u001B[31mNameError\u001B[39m: name 'stacked_model' is not defined"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T09:15:05.463847Z",
     "start_time": "2025-10-28T22:40:14.854560Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "73f7370b68e00151",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2baf9221b03a37fc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T09:15:05.474228100Z",
     "start_time": "2025-10-28T22:40:14.863217Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "24332ec4db42523e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b0135d56e0a2339b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "dd428530f7d40011"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2fc3f39679db14d5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
